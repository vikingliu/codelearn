{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c48607-2046-4fde-8626-1e48a04b910b",
   "metadata": {},
   "source": [
    "# 回归任务\n",
    "\n",
    "<img src=\"./data/img/L1_L2_loss.png\" style=\"zoom:50%\"/>\n",
    "\n",
    "## L1范数损失 L1Loss\n",
    "在机器学习和深度学习中，L1范数损失通常用于正则化和稀疏性推动。通过最小化L1范数损失，可以鼓励模型产生稀疏的权重或特征选择，因为L1范数倾向于将一些权重或特征设置为零。\n",
    ">$ J = loss(y, \\hat y) = \\sum|y - \\hat y|$    \n",
    "> $ \\frac {\\partial J} {\\partial \\hat y} = N_1 - N_2,  N_2表示 y≥\\hat y的个数，N_1表示 y<\\hat y的个数 $   \n",
    "> torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "应用场景：回归任务，简单的模型，由于神经网络通常是解决复杂问题，所以很少使用。\n",
    "\n",
    "\n",
    "## 均方误差（L2 Loss)损失 MSELoss\n",
    "计算 output 和 target 之差的均方差。   \n",
    "MSE损失的优点是对较大的预测误差有较高的惩罚，因为差异的平方放大了较大的误差。同时，MSE损失在数学性质上也比较好，易于计算和求导。\n",
    ">$ J = loss(y, \\hat y) = \\frac {1}{n} \\sum \\frac {1}{2}(y - \\hat y)^2$    \n",
    ">$ \\frac {\\partial J} {\\partial \\hat y} =  \\frac {1}{n} \\sum (\\hat y - y)$  \n",
    ">torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "应用场景：回归任务，数值特征不大问，题维度不高\n",
    "\n",
    "## 平滑版L1损失 SmoothL1Loss\n",
    "当预测值和ground truth差别较小的时候（绝对值差小于1），其实使用的是L2 Loss；而当差别大的时候，是L1 Loss的平移。\n",
    ">torch.nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "应用场景：回归任务, 特征中有较大的数值，适合大多数问题\n",
    "\n",
    "reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde016c-bc8a-47db-842e-1f8087c0e801",
   "metadata": {},
   "source": [
    "# 分类任务\n",
    "\n",
    "## 交叉熵损失 CrossEntropyLoss\n",
    "当训练有 C 个类别的分类问题时很有效. 可选参数 weight 必须是一个1维 Tensor, 权重将被分配给各个类别. 对于不平衡的训练集非常有效。\n",
    "在多分类任务中，经常采用 softmax 激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要 softmax激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算 loss。\n",
    ">$ J = loss(y, \\hat y) = - \\sum_c y_c\\log(\\hat y_c)$   \n",
    ">$ \\hat y_i = softmax(z_i) = \\frac {e^{z_i}}{\\sum_c e^{z_c}}$   \n",
    ">$ \\frac {\\partial J} {\\partial Z} = \\hat y - y$  \n",
    "torch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')   \n",
    "weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensor   \n",
    "ignore_index (int, optional) – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度。\n",
    "\n",
    "应用场景：多分类\n",
    "\n",
    "##  KL 散度（相对熵）损失 KLDivLoss\n",
    "计算 input 和 target 之间的 KL 散度。KL 散度可用于衡量不同的连续分布之间的距离, 在连续的输出分布的空间上(离散采样)上进行直接回归时 很有效.   \n",
    "KL散度通常用于无监督学习任务中，如聚类、降维和生成模型等。在这些任务中，我们没有相应的标签信息，因此无法使用交叉熵来评估模型的性能，所以需要一种方法来衡量模型预测的分布和真实分布之间的差异，这时就可以使用KL散度来衡量模型预测的分布和真实分布之间的差异。   \n",
    "需要注意的是，KL散度损失不具有对称性，即KLDivLoss(P, Q)与KLDivLoss(Q, P)的值可能不相等。因此，在实际应用中，我们通常将KL散度损失与交叉熵损失（CrossEntropyLoss）结合使用，以获得更好的效果。\n",
    "\n",
    ">$ J = loss(\\hat y, y) = \\sum_c y_c(-\\log\\hat y_c) - \\sum_c y_c(-\\log y_c) = \\sum_c y_c \\log(\\frac {y_c}{\\hat y})$     \n",
    ">$ \\hat y_i = softmax(z_i) = \\frac {e^{z_i}}{\\sum_c e^{z_c}}$    \n",
    ">$ - \\sum_c y_c(-\\log y_c)  常数， 所以  \\frac {\\partial J} {\\partial Z} = \\hat y - y$    \n",
    ">torch.nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "应用场景：无监督学习，如聚类、降维和生成模型等\n",
    "\n",
    "## 二进制交叉熵损失 BCELoss\n",
    "BCE损失函数的优点是在二分类问题中比较直观且易于计算(多分类的特殊形式）。它对于模型预测接近真实标签的情况有较低的损失，同时对于模型预测明显错误的情况有较高的惩罚。\n",
    "> $ J = loss(\\hat y, y) = -(y * \\log \\hat y + (1-y)*\\log(1-\\hat y))$   \n",
    "> $ \\hat y_i = sigmoid(z_i) = \\sigma (z_i) = \\frac {e^{z_i}}{1 + e^{z_i}}$   \n",
    "> $ \\frac {\\partial J} {\\partial Z} = \\hat y - y$   \n",
    "> torch.nn.BCELoss(weight=None, reduction='mean')\n",
    "\n",
    "应用场景：二分类\n",
    "\n",
    "## BCEWithLogitsLoss\n",
    "BCEWithLogitsLoss损失函数把 Sigmoid 层集成到了 BCELoss 类中。   \n",
    "与BCELoss不同的是，BCEWithLogitsLoss直接在内部应用sigmoid函数，省去了手动应用sigmoid函数的步骤。\n",
    "\n",
    ">$ J = BCEWithLogitsLoss(y, \\hat y) = -[y \\log(\\sigma(\\hat y)) + (1 - y) \\log(1 - \\sigma(\\hat y))]$   \n",
    ">$ \\sigma(\\hat y) = sigmoid(\\hat y), $   \n",
    ">$ \\frac {\\partial J}{\\partial \\hat y} = \\sigma(\\hat y) - y$   \n",
    ">torch.nn.BCEWithLogitsLoss(weight=None, reduction='mean', pos_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae396a4-7179-45b3-b2e4-e7489c4b53e3",
   "metadata": {},
   "source": [
    "# 度量学习(metric learning)\n",
    "## MarginRankingLoss\n",
    "MarginRankingLoss（边际排序损失）是一种用于学习排序模型的损失函数，常用于训练具有排序目标的模型，例如排序任务、排序推荐系统等。\n",
    "\n",
    "MarginRankingLoss的目标是将正样本的预测得分（正例）与负样本的预测得分（负例）之间的差异最大化，同时保持一定的边际（margin）。这样可以促使模型在预测时更好地区分正负样本，从而提高排序性能。\n",
    "\n",
    "min-batch, $ MarginRankingLoss = max(0, -y * (\\hat y_p - \\hat y_n) + margin)其中，y为标签，取值为1或-1，表示正样本或负样本；\\hat y_p为正样本的预测得分；\\hat y_n为负样本的预测得分；margin为边际，是一个预先指定的超参数。$\n",
    "\n",
    "> \n",
    ">torch.nn.MarginRankingLoss(margin=0.0, reduction='mean')\n",
    "\n",
    "应用场景：孪生（Siamese）网络，GAN，排名任务，开源实现和实例非常少\n",
    "\n",
    "## HingeEmbeddingLoss\r\n",
    "HingeEmbeddingLoss（铰链嵌入损失）是一种用于训练支持向量机（Support Vector Machine，SVM）模型的损失函数，常用于二分类任务。它的目标是鼓励模型将正负样本分开，并在一定程度上惩罚分类错误。266\n",
    "\n",
    "$ J = HingeEmbeddingLoss(y, \\hat y) = max(0, margin - y * \\hat y) ,y ∈\\{-1, 1\\}$  \n",
    "$\n",
    "J = HingeEmbeddingLoss(y, \\hat y)=\\begin{cases}\n",
    "max(0, margin - \\hat y) & y == 1 \\\\\n",
    "max(0, margin + \\hat y) & y == -1\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$ \n",
    "\\frac {\\partial J}{\\partial \\hat y} = \\begin{cases}\n",
    "1 & y == -1 \\text{ and } margin > -\\hat y \\\\\n",
    "0  & margin < y * \\hat y \\\\\n",
    "-1 & y == 1 \\text{ and } margin > \\hat y\n",
    "\\end{cases}\n",
    "$  \n",
    "HingeEmbeddingLoss 是一种损失函数，通常用于训练具有嵌入向量（embedding vectors）的模型，如Siamese网络，Triplet网络等，以学习如何度量样本之间的相似性或差异性。这种损失函数通常在对比学习任务中使用，其中目标是确保正样本对（相似样本）之间的距离小于负样本对（不相似样本）之间的距离，同时还考虑到了一个预定义的边距（margin），这有助于将相似性和差异性信息嵌入到学习的嵌入空间中。\n",
    "\n",
    "min-batch, $x_i = 1 - cosine(a,b)$\n",
    "\n",
    "$\n",
    " J = loss(x_i,y) = \\begin{cases}\n",
    " x_i  &  y == 1\\\\\n",
    " max(0, margin - x_i) &  y == -1\n",
    " \\end{cases}\n",
    "$\n",
    "\n",
    "$ \n",
    " \\frac {\\partial J}{\\partial _i } = \\begin{cases}\n",
    " 1 & y == 1 \\\\\n",
    " 0 & y == -1 \\text{ and  }margin< x_i \\\\\n",
    " -1 & y== -1 \\text{ and  }margin> x_i \n",
    " \\end{cases}\n",
    "$   \n",
    " \n",
    "> torch.nn.HingeEmbeddingLoss(margin=1.0,  reduction='mean')\n",
    "\n",
    "应用场景：siamese net或者Triplet net，非线形Embedding，半监督学习，监测两个输入的相似性或者不相似性\n",
    "\n",
    "\n",
    "## cosine 损失 CosineEmbeddingLoss\n",
    "余弦损失函数，余弦函数常常用于评估两个向量的相似性，两个向量的余弦值越高，则相似性越高。\n",
    "\n",
    "$\n",
    " loss(x_1, x_2) = \\begin{cases}\n",
    "1 - \\cos(x_1, x_2) & {y == 1}   \\\\\n",
    "max(0, \\cos(x_1, x_2) - margin) & {y == -1}\n",
    "\\end{cases}\n",
    "$\n",
    ">torch.nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')\n",
    "\n",
    "应用场景：非线形Embedding，半监督学习，监测两个输入的相似性或者不相似性\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db4779a-e2bb-413f-ac9d-07f62d24dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "0.4938560426235199\n",
      "tensor([0.0000, 1.0821, 0.0000, 1.0337, 1.0798, 0.0000, 1.0582, 0.0000, 0.8795,\n",
      "        0.0000, 1.1377, 0.0000, 0.9727, 1.0088, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9941, 1.0539, 0.0000, 0.0000, 0.0000, 1.1907, 0.9647, 0.8875,\n",
      "        0.8585, 0.9471, 0.0000, 0.0000, 0.9677, 0.0000, 0.0000, 0.0000, 0.8393,\n",
      "        0.0000, 0.9900, 1.1510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.9491, 0.9202, 0.0000, 0.9338, 1.0044, 0.0000, 1.1716, 1.0480, 0.8654,\n",
      "        0.8302, 0.0000, 0.8969, 0.0000, 0.0000, 1.0293, 0.0000, 1.1107, 0.8257,\n",
      "        0.9162, 1.0796, 1.0330, 0.0000, 0.9933, 0.0000, 0.0000, 1.0066, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.9410, 0.8609, 1.0060, 0.0000, 0.8454, 0.0000,\n",
      "        1.0362, 0.0000, 1.0253, 1.0560, 1.0759, 0.9888, 0.0000, 1.0147, 0.8566,\n",
      "        0.9453, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9874, 0.0000, 0.0000,\n",
      "        1.0352], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "\n",
    "torch.manual_seed(20)\n",
    "hinge_loss = nn.HingeEmbeddingLoss(margin=0.2)\n",
    "a = torch.randn(100, 128, requires_grad=True)\n",
    "b = torch.randn(100, 128, requires_grad=True)\n",
    "x = 1 - torch.cosine_similarity(a, b)\n",
    "# 定义a与b之间的距离为x\n",
    "print(x.size())\n",
    "y = 2 * torch.empty(100).random_(2) - 1\n",
    "output = hinge_loss(x, y)\n",
    "print(output.item())\n",
    "\n",
    "hinge_loss = nn.HingeEmbeddingLoss(margin=0.2, reduction=\"none\")\n",
    "output = hinge_loss(x, y)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725940d7-d74b-4698-9fff-2f27da8471e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 多标签分类损失 MultiLabelMarginLoss\n",
    "\n",
    "\n",
    "torch.nn.MultiLabelMarginLoss(reduction='mean')\n",
    "\n",
    "\n",
    "## 2分类的logistic损失 SoftMarginLoss\n",
    "\n",
    "torch.nn.SoftMarginLoss(reduction='mean')\n",
    "\n",
    "## 多标签 one-versus-all 损失 MultiLabelSoftMarginLoss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
