{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c0758c-d583-41a8-b065-5f610f7f7921",
   "metadata": {},
   "source": [
    "<div id=\"content_views\" class=\"htmledit_views\">\n",
    "                    <h4><a name=\"t0\"></a><strong>CV-CNN</strong></h4> \n",
    "<ul><li><a href=\"https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\" rel=\"nofollow\" title=\"AlexNet\">AlexNet</a>（2012）：深度学习热潮的奠基作</li><li><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" rel=\"nofollow\" title=\"VGG\">VGG</a>（2014）：使用 3x3 卷积构造更深的网络</li><li><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" rel=\"nofollow\" title=\"GoogleNet\">GoogleNet</a>（2014）：使用并行架构构造更深的网络</li><li><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" rel=\"nofollow\" title=\"ResNet\">ResNet</a>（2015）：构建深层网络的残差连接</li><li><a href=\"https://arxiv.org/pdf/1704.04861.pdf\" rel=\"nofollow\" title=\"MobileNet\">MobileNet</a>（2017）：适合终端设备的小CNN</li><li><a href=\"https://arxiv.org/pdf/1905.11946.pdf\" rel=\"nofollow\" title=\"EfficientNet\">EfficientNet</a>（2019）：通过架构搜索得到的CNN</li><li><a href=\"https://arxiv.org/pdf/2110.07641.pdf\" rel=\"nofollow\" title=\"Non-deep networks\">Non-deep networks</a>（2021）：让不深的网络也能在ImageNet刷到SOTA</li></ul> \n",
    "<h4><a name=\"t1\"></a></h4> \n",
    "<h4><a name=\"t2\"></a><strong>CV-Object Detection</strong></h4> \n",
    "<ul><li><a href=\"https://arxiv.org/pdf/1311.2524v5.pdf\" rel=\"nofollow\" title=\"R-CNN\">R-CNN</a>（2014）：Two-stage</li><li><a href=\"http://arxiv.org/abs/1504.08083v2\" rel=\"nofollow\" title=\"Fast R-CNN\">Fast R-CNN</a>（2015）</li><li><a href=\"http://arxiv.org/abs/1506.01497v3\" rel=\"nofollow\" title=\"Faster R-CNN\">Faster R-CNN</a>（2015）</li><li><a href=\"http://arxiv.org/abs/1512.02325v5\" rel=\"nofollow\" title=\"SSD\">SSD</a>（2016）：Single stage</li><li><a href=\"http://arxiv.org/abs/1506.02640v5\" rel=\"nofollow\" title=\"YOLO\">YOLO</a>（2016）</li><li><a href=\"http://arxiv.org/abs/1703.06870v3\" rel=\"nofollow\" title=\"Mask R-CNN\">Mask R-CNN</a>（2017）</li><li><a href=\"http://arxiv.org/abs/1612.08242v1\" rel=\"nofollow\" title=\"YOLOv2\">YOLOv2</a>（2017）</li><li><a href=\"http://arxiv.org/abs/1804.02767v1\" rel=\"nofollow\" title=\"YOLOv3\">YOLOv3</a>（2018）</li><li><a href=\"https://arxiv.org/pdf/1904.07850.pdf\" rel=\"nofollow\" title=\"CenterNet\">CenterNet</a>（2019）：Anchor free</li><li><a href=\"https://arxiv.org/pdf/2005.12872.pdf\" rel=\"nofollow\" title=\"DETR\">DETR</a>（2020）：Transformer</li></ul> \n",
    "<p></p> \n",
    "<h4><a name=\"t3\"></a><strong>CV-</strong>Comparative Learning</h4> \n",
    "<ul><li><a href=\"https://arxiv.org/pdf/1805.01978.pdf\" rel=\"nofollow\" title=\"InstDisc\">InstDisc</a>（2018）：提出实例判别和memory bank做对比学习</li><li><a href=\"https://arxiv.org/pdf/1807.03748.pdf\" rel=\"nofollow\" title=\"CPC\">CPC</a>（2018）：对比预测编码，图像语音文本强化学习全都能做</li><li><a href=\"https://arxiv.org/pdf/1904.03436.pdf\" rel=\"nofollow\" title=\"InvaSpread\">InvaSpread</a>（2019）：一个编码器的端到端对比学习</li><li><a href=\"https://arxiv.org/pdf/1906.05849.pdf\" rel=\"nofollow\" title=\"CMC\">CMC</a>（2019）：多视角下的对比学习</li><li><a href=\"https://arxiv.org/pdf/1911.05722.pdf\" rel=\"nofollow\" title=\"MoCov1\">MoCov1</a>（2020）：无监督训练</li><li><a href=\"https://arxiv.org/pdf/2002.05709.pdf\" rel=\"nofollow\" title=\"SimCLRv1\">SimCLRv1</a>（2020）：简单的对比学习 (数据增强 + MLP head + 大batch训练久)</li><li><a href=\"https://arxiv.org/pdf/2003.04297.pdf\" rel=\"nofollow\" title=\"MoCov2\">MoCov2</a>（2020）：MoCov1 + improvements from SimCLRv1</li><li><a href=\"https://arxiv.org/pdf/2006.10029.pdf\" rel=\"nofollow\" title=\"SimCLRv2\">SimCLRv2</a>（2020）：大的自监督预训练模型很适合做半监督学习</li><li><a href=\"https://arxiv.org/pdf/2006.07733.pdf\" rel=\"nofollow\" title=\"BYOL\">BYOL</a>（2020）：不需要负样本的对比学习</li><li><a href=\"https://arxiv.org/pdf/2006.09882.pdf\" rel=\"nofollow\" title=\"SWaV\">SWaV</a>（2020）：聚类对比学习</li><li><a href=\"https://arxiv.org/pdf/2011.10566.pdf\" rel=\"nofollow\" title=\"SimSiam\">SimSiam</a>（2020）：化繁为简的孪生表征学习</li><li><a href=\"https://arxiv.org/pdf/2104.02057.pdf\" rel=\"nofollow\" title=\"MoCov3\">MoCov3</a>（2021）：如何更稳定的自监督训练ViT</li><li><a href=\"https://arxiv.org/pdf/2104.14294.pdf\" rel=\"nofollow\" title=\"DINO\">DINO</a>（2021）：transformer加自监督在视觉</li></ul> \n",
    "<p></p> \n",
    "<h4><a name=\"t4\"></a><strong>CV-Transformer</strong></h4> \n",
    "<ul><li><a href=\"https://arxiv.org/pdf/2010.11929.pdf\" rel=\"nofollow\" title=\"ViT\">ViT</a>（2020）：Transformer杀入CV界</li><li><a href=\"https://arxiv.org/pdf/2103.14030.pdf\" rel=\"nofollow\" title=\"Swin Transformer\">Swin Transformer</a>（2021）：多层次的Vision Transformer</li><li><a href=\"https://arxiv.org/pdf/2105.01601.pdf\" rel=\"nofollow\" title=\"MLP-Mixer\">MLP-Mixer</a>（2021）：使用MLP替换self-attention</li><li><a href=\"https://arxiv.org/pdf/2111.06377.pdf\" rel=\"nofollow\" title=\"MAE\">MAE</a>（2021）：BERT的CV版</li></ul> \n",
    "<p></p> \n",
    "<h4><a name=\"t5\"></a><strong>CV-Video Understanding</strong></h4> \n",
    "<ul><li><a href=\"https://cs.stanford.edu/people/karpathy/deepvideo/\" rel=\"nofollow\" title=\"DeepVideo\">DeepVideo</a>（2014）：提出sports1M数据集，用深度学习做视频理解</li><li><a href=\"https://arxiv.org/pdf/1406.2199.pdf\" rel=\"nofollow\" title=\"Two-stream\">Two-stream</a>（2014）：引入光流做时序建模，神经网络首次超越手工特征</li><li><a href=\"https://arxiv.org/pdf/1412.0767.pdf\" rel=\"nofollow\" title=\"C3D\">C3D</a>（2014）：比较深的3D-CNN做视频理解</li><li><a href=\"https://arxiv.org/pdf/1503.08909.pdf\" rel=\"nofollow\" title=\"Beyond-short-snippets\">Beyond-short-snippets</a>（2015）：尝试使用LSTM</li><li><a href=\"https://arxiv.org/pdf/1604.06573.pdf\" rel=\"nofollow\" title=\"Convolutional fusion\">Convolutional fusion</a>（2016）：early fusion来加强时空间建模</li><li><a href=\"https://arxiv.org/pdf/1608.00859.pdf\" rel=\"nofollow\" title=\"TSN\">TSN</a>（2017）：视频分段建模，bag of tricks in video</li><li><a href=\"https://arxiv.org/pdf/1705.07750.pdf\" rel=\"nofollow\" title=\"I3D\">I3D</a>（2017）：提出Kinetics数据集，膨胀2D网络到3D，开启3D-CNN时代</li><li><a href=\"https://arxiv.org/pdf/1711.11248.pdf\" rel=\"nofollow\" title=\"R2+1D\">R2+1D</a>（2017）：拆分3D卷积核，使3D网络容易优化</li><li><a href=\"https://arxiv.org/pdf/1711.07971.pdf\" rel=\"nofollow\" title=\"Non-local\">Non-local</a>（2017）：引入自注意力做视觉问题</li><li><a href=\"https://arxiv.org/pdf/1812.03982.pdf\" rel=\"nofollow\" title=\"SlowFast\">SlowFast</a>（2018）：快慢两支提升效率</li><li><a href=\"https://arxiv.org/pdf/2102.05095.pdf\" rel=\"nofollow\" title=\"TimeSformer\">TimeSformer</a>（2021）：视频中第一个引入transformer，开启video transformer时代</li></ul> \n",
    "<p></p> \n",
    "<h4><a name=\"t6\"></a><strong>NLP-Transform</strong></h4> \n",
    "<ul><li><a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\" title=\"Transformer\">Transformer</a>：继MLP、CNN、RNN后的第四大类构架</li><li><a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" rel=\"nofollow\" title=\"GPT\">GPT</a>：使用Transformer解码器做预训练</li><li><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\" title=\"BERT\">BERT</a>：Transformer一统NLP的开始</li><li><a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\" rel=\"nofollow\" title=\"GPT-2\">GPT-2</a>：更大的 GPT 模型，朝着zero-shot learning迈了一大步</li><li><a href=\"https://arxiv.org/abs/2005.14165\" rel=\"nofollow\" title=\"GPT-3\">GPT-3</a>：100倍更大的 GPT-2，few-shot learning效果显著</li></ul> \n",
    "<p></p> \n",
    "<h4><a name=\"t7\"></a><strong>GAN</strong></h4> \n",
    "<ul><li><a href=\"https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\" rel=\"nofollow\" title=\"GAN\">GAN</a>（2014）：生成模型的开创工作</li><li><a href=\"https://arxiv.org/pdf/1511.06434.pdf\" rel=\"nofollow\" title=\"DCGAN\">DCGAN</a>（2015）：使用CNN的GAN</li><li><a href=\"https://arxiv.org/pdf/1611.07004.pdf\" rel=\"nofollow\" title=\"pix2pix\">pix2pix</a>（2016）</li><li><a href=\"https://arxiv.org/pdf/1609.04802.pdf\" rel=\"nofollow\" title=\"SRGAN\">SRGAN</a>（2016）：图片超分辨率</li><li><a href=\"https://arxiv.org/abs/1701.07875\" rel=\"nofollow\" title=\"WGAN\">WGAN</a>（2017）：训练更加容易</li><li><a href=\"https://arxiv.org/abs/1703.10593\" rel=\"nofollow\" title=\"CycleGAN\">CycleGAN</a>（2017）</li><li><a href=\"https://arxiv.org/abs/1812.04948\" rel=\"nofollow\" title=\"StyleGAN\">StyleGAN</a>（2018）</li><li><a href=\"https://arxiv.org/pdf/1912.04958.pdf\" rel=\"nofollow\" title=\"StyleGAN2\">StyleGAN2</a>（2019）</li><li><a href=\"https://arxiv.org/pdf/2006.11239.pdf\" rel=\"nofollow\" title=\"DDPM\">DDPM</a>（2020）：Diffusion Models</li><li><a href=\"https://arxiv.org/pdf/2102.09672.pdf\" rel=\"nofollow\" title=\"Improved DDPM\">Improved DDPM</a>（2021）：改进的 DDPM</li><li><a href=\"https://arxiv.org/pdf/2105.05233.pdf\" rel=\"nofollow\" title=\"Guided Diffusion Models\">Guided Diffusion Models</a>（2021）：号称超越 GAN</li><li><a href=\"https://arxiv.org/pdf/2106.12423.pdf\" rel=\"nofollow\" title=\"StyleGAN3\">StyleGAN3</a>（2021）</li><li><a href=\"https://arxiv.org/pdf/2204.06125.pdf\" rel=\"nofollow\" title=\"DALL.E 2\">DALL.E 2</a>（2022）：CLIP + Diffusion models，文本生成图像新高度</li></ul> \n",
    "<p></p> \n",
    "<h4><a name=\"t8\"></a>多模态</h4> \n",
    "<ul><li><a href=\"https://openai.com/blog/clip/\" rel=\"nofollow\" title=\"CLIP\">CLIP</a>（2021）：图片和文本之间的对比学习</li><li><a href=\"https://arxiv.org/pdf/2102.03334.pdf\" rel=\"nofollow\" title=\"ViLT\">ViLT</a>（2021）：第一个摆脱了目标检测的视觉文本模型</li><li><a href=\"https://arxiv.org/pdf/2104.13921.pdf\" rel=\"nofollow\" title=\"ViLD\">ViLD</a>（2021）：CLIP蒸馏帮助开集目标检测</li><li><a href=\"https://arxiv.org/pdf/2112.03857.pdf\" rel=\"nofollow\" title=\"GLIP\">GLIP</a>（2021）：联合目标检测和文本定位</li><li><a href=\"https://arxiv.org/pdf/2104.08860.pdf\" rel=\"nofollow\" title=\"CLIP4Clip\">CLIP4Clip</a>（2021）：拿CLIP直接做视频文本retrieval</li><li><a href=\"https://arxiv.org/pdf/2109.08472.pdf\" rel=\"nofollow\" title=\"ActionCLIP\">ActionCLIP</a>（2021）：用多模态对比学习有监督的做视频动作分类</li><li><a href=\"https://arxiv.org/pdf/2112.02413.pdf\" rel=\"nofollow\" title=\"PointCLIP\">PointCLIP</a>（2021）：3D变2D，巧妙利用CLIP做点云</li><li><a href=\"https://arxiv.org/pdf/2201.03546.pdf\" rel=\"nofollow\" title=\"LSeg\">LSeg</a>（2022）：有监督的开集分割</li><li><a href=\"https://arxiv.org/pdf/2202.11094.pdf\" rel=\"nofollow\" title=\"GroupViT\">GroupViT</a>（2022）：只用图像文本对也能无监督做分割</li><li><a href=\"https://arxiv.org/pdf/2202.05822.pdf\" rel=\"nofollow\" title=\"CLIPasso\">CLIPasso</a>（2022）：CLIP跨界生成简笔画</li><li><a href=\"https://arxiv.org/pdf/2207.01077.pdf\" rel=\"nofollow\" title=\"DepthCLIP\">DepthCLIP</a>（2022）：用文本跨界估计深度</li></ul> \n",
    "<p></p> \n",
    "<p>Reference：<a href=\"https://github.com/mli/paper-reading\" title=\"GitHub - mli/paper-reading: 深度学习经典、新论文逐段精读\">GitHub - mli/paper-reading: 深度学习经典、新论文逐段精读</a></p>\n",
    "                </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811ba39-794e-45c7-bb55-58f4a10b940f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
