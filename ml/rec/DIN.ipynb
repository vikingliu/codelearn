{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b2678b-889e-402a-a1a6-57726057090d",
   "metadata": {},
   "source": [
    "# DIN Base\n",
    "\n",
    "DIN模型的输入特征大致上分为了三类： Dense(连续型), Sparse(离散型), VarlenSparse(变长离散型)，也就是指的上面的历史行为数据。而不同的类型特征也就决定了后面处理的方式会不同：\n",
    "* Dense型特征：由于是数值型了，这里为每个这样的特征建立Input层接收这种输入， 然后拼接起来先放着，等离散的那边处理好之后，和离散的拼接起来进DNN\n",
    "* Sparse型特征，为离散型特征建立Input层接收输入，然后需要先通过embedding层转成低维稠密向量，然后拼接起来放着，等变长离散那边处理好之后， 一块拼起来进DNN， 但是这里面要注意有个特征的embedding向量还得拿出来用，就是候选商品的embedding向量，这个还得和后面的计算相关性，对历史行为序列加权。\n",
    "* VarlenSparse型特征：这个一般指的用户的历史行为特征，变长数据， 首先会进行padding操作成等长， 然后建立Input层接收输入，然后通过embedding层得到各自历史行为的embedding向量， 拿着这些向量与上面的候选商品embedding向量进入AttentionPoolingLayer去对这些历史行为特征加权合并，最后得到输出。\n",
    "\n",
    "\n",
    "DIN 模型的应用场景是阿里最典型的电商广告推荐，有大量的用户历史行为信息（历史购买过得商品或类别信息）。对于付了广告费的商品，阿里会根据模型预测的点击率高低，把合适的广告商品推荐给合适的用户，所以 DIN 模型本质上是一个点击率预估模型。\n",
    "\n",
    "下面的图 1 就是 DIN 的基础模型 Base Model。我们可以看到，Base Model 是一个典型的 Embedding MLP 的结构。它的输入特征有用户属性特征（User Proflie Features）、用户行为特征（User Behaviors）、候选广告特征（Candidate Ad）和场景特征（Context Features）。\n",
    "\n",
    "<img src=\"../data/img/DIN_base_model.webp\" style=\"zoom:50%\" />\n",
    "\n",
    "\n",
    "# DIN \n",
    "\n",
    "<img src=\"../data/img/DIN.webp\" style=\"zoom:50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa5432-23cc-4521-8eed-4c20590689b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationUnit(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, dims=[36], activation=\"dice\", use_softmax=False):\n",
    "        super(ActivationUnit, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.use_softmax = use_softmax\n",
    "        # Dice(36)\n",
    "        self.attention = MLP(4 * self.emb_dim, dims=dims, activation=activation)\n",
    "\n",
    "    def forward(self, history, target):\n",
    "        seq_length = history.size(1)\n",
    "        target = target.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        # Concat\n",
    "        att_input = torch.cat([target, history, target - history, target * history], dim=-1)  \n",
    "        # Dice(36)\n",
    "        att_weight = self.attention(att_input.view(-1, 4 * self.emb_dim))  \n",
    "        # Linear(1)\n",
    "        att_weight = att_weight.view(-1, seq_length)\n",
    "        if self.use_softmax:\n",
    "            att_weight = att_weight.softmax(dim=-1)\n",
    "        # (batch_size,emb_dim)\n",
    "        output = (att_weight.unsqueeze(-1) * history).sum(dim=1)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64df55f-23d6-4f73-a53b-a1b623b8ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi Layer Perceptron Module, it is the most widely used module for \n",
    "    learning feature. Note we default add `BatchNorm1d` and `Activation` \n",
    "    `Dropout` for each `Linear` Module.\n",
    "\n",
    "    Args:\n",
    "        input dim (int): input size of the first Linear Layer.\n",
    "        output_layer (bool): whether this MLP module is the output layer. If `True`, then append one Linear(*,1) module. \n",
    "        dims (list): output size of Linear Layer (default=[]).\n",
    "        dropout (float): probability of an element to be zeroed (default = 0.5).\n",
    "        activation (str): the activation function, support `[sigmoid, relu, prelu, dice, softmax]` (default='relu').\n",
    "\n",
    "    Shape:\n",
    "        - Input: `(batch_size, input_dim)`\n",
    "        - Output: `(batch_size, 1)` or `(batch_size, dims[-1])`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_layer=True, dims=[], dropout=0, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        layers = list()\n",
    "        for i_dim in dims:\n",
    "            layers.append(nn.Linear(input_dim, i_dim))\n",
    "            layers.append(nn.BatchNorm1d(i_dim))\n",
    "            layers.append(activation_layer(activation))\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            input_dim = i_dim\n",
    "        if output_layer:\n",
    "            layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a3b17-0f2a-4b39-8b0d-3a6c15a288a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.modules.activation import Sigmoid\n",
    "\n",
    "class DIN(nn.Module):\n",
    "    def __init__(self, candidate_movie_num, recent_rate_num, user_profile_num, context_feature_num, candidate_movie_dict, \n",
    "            recent_rate_dict, user_profile_dict, context_feature_dict, history_num, embed_dim, activation_dim, hidden_dim=[128, 64]):\n",
    "        super().__init__()\n",
    "        self.candidate_vocab_list = list(candidate_movie_dict.values())\n",
    "        self.recent_rate_list = list(recent_rate_dict.values())\n",
    "        self.user_profile_list = list(user_profile_dict.values())\n",
    "        self.context_feature_list = list(context_feature_dict.values())\n",
    "        self.embed_dim = embed_dim\n",
    "        self.history_num = history_num\n",
    "        # candidate_embedding_layer \n",
    "        self.candidate_embedding_list = nn.ModuleList([nn.Embedding(vocab_size, embed_dim) for vocab_size in self.candidate_vocab_list])\n",
    "        # recent_rate_embedding_layer\n",
    "        self.recent_rate_embedding_list = nn.ModuleList([nn.Embedding(vocab_size, embed_dim) for vocab_size in self.recent_rate_list])\n",
    "        # user_profile_embedding_layer\n",
    "        self.user_profile_embedding_list = nn.ModuleList([nn.Embedding(vocab_size, embed_dim) for vocab_size in self.user_profile_list])\n",
    "        # context_embedding_list\n",
    "        self.context_embedding_list = nn.ModuleList([nn.Embedding(vocab_size, embed_dim) for vocab_size in self.context_feature_list])\n",
    "\n",
    "        # activation_unit\n",
    "        self.activation_unit = nn.Sequential(nn.Linear(4*embed_dim, activation_dim), \n",
    "                                            nn.PReLU(),\n",
    "                                            nn.Linear(activation_dim, 1),\n",
    "                                            nn.Sigmoid())\n",
    "        \n",
    "        # self.dnn_part\n",
    "        self.dnn_input_dim = len(self.candidate_embedding_list) * embed_dim + candidate_movie_num - len(\n",
    "            self.candidate_embedding_list) + embed_dim + len(self.user_profile_embedding_list) * embed_dim + \\\n",
    "            user_profile_num - len(self.user_profile_embedding_list) + len(self.context_embedding_list) * embed_dim \\\n",
    "            + context_feature_num - len(self.context_embedding_list)\n",
    "\n",
    "        self.dnn = nn.Sequential(nn.Linear(self.dnn_input_dim, hidden_dim[0]),\n",
    "                             nn.BatchNorm1d(hidden_dim[0]),\n",
    "                             nn.PReLU(),\n",
    "                             nn.Linear(hidden_dim[0], hidden_dim[1]),\n",
    "                             nn.BatchNorm1d(hidden_dim[1]),\n",
    "                             nn.PReLU(),\n",
    "                             nn.Linear(hidden_dim[1], 1),\n",
    "                             nn.Sigmoid())\n",
    "\n",
    "    def forward(self, candidate_features, recent_features, user_features, context_features):\n",
    "        bs = candidate_features.shape[0]\n",
    "        # candidate cate_feat embed\n",
    "        candidate_embed_features = []\n",
    "        for i, embed_layer in enumerate(self.candidate_embedding_list):\n",
    "            candidate_embed_features.append(embed_layer(candidate_features[:, i].long()))\n",
    "        candidate_embed_features = torch.stack(candidate_embed_features, dim=1).reshape(bs, -1).unsqueeze(1)\n",
    "        ## add candidate continous feat\n",
    "        candidate_continous_features = candidate_features[:, len(candidate_features):]\n",
    "        candidate_branch_features = torch.cat([candidate_continous_features.unsqueeze(1), candidate_embed_features], dim=2).repeat(1, self.history_num, 1)\n",
    "\n",
    "        # recent_rate  cate_feat embed\n",
    "        recent_embed_features = []\n",
    "        for i, embed_layer in enumerate(self.recent_rate_embedding_list):\n",
    "            recent_embed_features.append(embed_layer(recent_features[:, i].long()))\n",
    "        recent_branch_features = torch.stack(recent_embed_features, dim=1)\n",
    "        \n",
    "        # user_profile feat embed \n",
    "        user_profile_embed_features = []\n",
    "        for i, embed_layer in enumerate(self.user_profile_embedding_list):\n",
    "            user_profile_embed_features.append(embed_layer(user_features[:, i].long()))\n",
    "        user_profile_embed_features = torch.cat(user_profile_embed_features, dim=1)\n",
    "        ## add user_profile continous feat\n",
    "        user_profile_continous_features = user_features[:, len(self.user_profile_list):]\n",
    "        user_profile_branch_features = torch.cat([user_profile_embed_features, user_profile_continous_features], dim=1)\n",
    "\n",
    "        # context embed feat\n",
    "        context_embed_features = []\n",
    "        for i, embed_layer in enumerate(self.context_embedding_list):\n",
    "            context_embed_features.append(embed_layer(context_features[:, i].long()))\n",
    "        context_embed_features = torch.cat(context_embed_features, dim=1)\n",
    "        ## add context continous feat\n",
    "        context_continous_features = context_features[:, len(self.context_embedding_list):]\n",
    "        context_branch_features = torch.cat([context_embed_features, context_continous_features], dim=1)\n",
    "\n",
    "        # activation_unit\n",
    "        sub_unit_input = recent_branch_features - candidate_branch_features\n",
    "        product_unit_input = torch.mul(recent_branch_features, candidate_branch_features)\n",
    "        unit_input = torch.cat([recent_branch_features, candidate_branch_features, sub_unit_input, product_unit_input], dim=2)\n",
    "        # weight-pool\n",
    "        activation_unit_out = self.activation_unit(unit_input).repeat(1, 1, self.embed_dim)\n",
    "        recent_branch_pooled_features = torch.mean(torch.mul(activation_unit_out, recent_branch_features), dim=1)\n",
    "        # dnn part\n",
    "        dnn_input = torch.cat([candidate_branch_features[:, 0, :], recent_branch_pooled_features, user_profile_branch_features, context_branch_features], dim=1)\n",
    "        dnn_out = self.dnn(dnn_input)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673e2c6-1e51-49de-9555-a6d6a5472102",
   "metadata": {},
   "source": [
    "<img src=\"../data/img/din_build.webp\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcdb25-197e-4201-922f-62f36e373490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIN_Build(torch.nn.Module):\n",
    "    def __init__(self, features, history_features, target_features, mlp_params, attention_mlp_params):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.history_features = history_features\n",
    "        self.target_features = target_features\n",
    "        # 历史行为特征个数\n",
    "        self.num_history_features = len(history_features)\n",
    "        # 计算所有的dim\n",
    "        self.all_dims = sum([fea.embed_dim for fea in features + history_features + target_features])\n",
    "        \n",
    "        # 构建Embeding层\n",
    "        self.embedding = EmbeddingLayer(features + history_features + target_features)\n",
    "        # 构建注意力层\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [ActivationUnit(fea.embed_dim, **attention_mlp_params) for fea in self.history_features])\n",
    "        self.mlp = MLP(self.all_dims, activation=\"dice\", **mlp_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_x_features = self.embedding(x, self.features)\n",
    "        embed_x_history = self.embedding(x, self.history_features)\n",
    "        embed_x_target = self.embedding(x, self.target_features)\n",
    "        attention_pooling = []\n",
    "        for i in range(self.num_history_features):\n",
    "            attention_seq = self.attention_layers[i](embed_x_history[:, i, :, :], embed_x_target[:, i, :])\n",
    "            attention_pooling.append(attention_seq.unsqueeze(1)) \n",
    "        # SUM Pooling\n",
    "        attention_pooling = torch.cat(attention_pooling, dim=1)\n",
    "        # Concat & Flatten\n",
    "        mlp_in = torch.cat([\n",
    "            attention_pooling.flatten(start_dim=1),\n",
    "            embed_x_target.flatten(start_dim=1),\n",
    "            embed_x_features.flatten(start_dim=1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # 可传入[80, 200]\n",
    "        y = self.mlp(mlp_in)\n",
    "        \n",
    "        # 代码中使用的是sigmoid(1)+BCELoss，效果和论文中的DIN模型softmax(2)+CELoss类似\n",
    "        return torch.sigmoid(y.squeeze(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
