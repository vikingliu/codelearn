{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24bfda6-d8db-4a98-baab-607a900e6db7",
   "metadata": {},
   "source": [
    "## DCN全称Deep & Cross Network\n",
    "\n",
    "是谷歌和斯坦福大学在2017年提出的用于Ad Click Prediction的模型。DCN(Deep Cross Network)在学习特定阶数组合特征的时候效率非常高，而且同样不需要特征工程，引入的额外的复杂度也是微乎其微的。\n",
    "\n",
    "<img src=\"../data/img/dcn.webp\" style=\"zoom:50%\" />\n",
    "\n",
    "### Cross Network 计算公式\n",
    "<img src=\"../data/img/cross_network.png\" style=\"zoom:50%\" />\n",
    "\n",
    "<img src=\"../data/img/feature_cross.webp\" style=\"zoom:50%\" />\n",
    "\n",
    "### 参数个数\n",
    "$Cross 深度L_c层，X_0的维度 d, 每一层的参数W和b维度跟X一样， 总参数量 2 * d * L_c$\n",
    "  \n",
    "\n",
    "Deep层： MLP\n",
    "\n",
    "\n",
    "### Loss\n",
    "损失函数使用带正则项的log loss\n",
    "\n",
    "<img src=\"../data/img/dcn_loss.webp\" style=\"zoom:50%\" />\n",
    "\n",
    "\n",
    "\n",
    "### 泛化FM\n",
    "跟FM一样，DCN同样也是基于参数共享机制的，参数共享不仅仅使得模型更加高效而且使得模型可以泛化到之前没有出现过的特征组合，并且对噪声的抵抗性更加强。\n",
    "FM是一个非常浅的结构，并且限制在表达二阶组合特征上，DeepCrossNetwork(DCN)把这种参数共享的思想从一层扩展到多层，并且可以学习高阶的特征组合。但是和FM的高阶版本的变体不同，DCN的参数随着输入维度的增长是线性增长的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c473efd-23e5-4f76-bf45-397013926e38",
   "metadata": {},
   "source": [
    "## DCN V2\n",
    "\n",
    "DCN-v2模型相比于DCN模型的优势在于：\n",
    "\n",
    "1. 对于交互特征的表达能力更强；\n",
    "2. DCN-v2模型的结构比较简单，比较适合作为复杂模型中的一个模型算子来引入使用；\n",
    "3. 在特征交互信息的学习上非常高效，尤其在与低秩矩阵架构结合使用时更加高效。\n",
    "\n",
    "<img src=\"../data/img/dcn_v2.jpeg\" sytle=\"zoom:50%\" />\n",
    "\n",
    "1. Embedding Layer：embedding层，主要是用于对sparse特征和dense特征预处理；\n",
    "2. Cross Network：交叉网络，这也是DCN-v2模型的核心部分了；\n",
    "3. Deep Network：MLP模块，常见的DNN结构；\n",
    "4. Deep and Cross Combination：交叉模块和MLP模块的组合，这里有并行和stack的组合方式；\n",
    "5. Cost-Effective Mixture of Low-Rank DCN：基于对矩阵地址特性的分析，进一步平衡了模型的性能和效果。\n",
    "\n",
    "### Cross Network\n",
    "\n",
    "<img src=\"../data/img/cross_network_v2.webp\" sytle=\"zoom:50%\" />\n",
    "\n",
    "\n",
    "### Cost-Effective Mixture of Low-Rank DCN\n",
    "\n",
    "在实际工业界中，模型复杂度容易被计算资源和响应延迟这些条件所限制，一般都会在计算开销和模型精度之间做trade-off。在数学上，低秩方法是用来减小矩阵计算开销的常用方法，它是将一个稠密矩阵分解成两个高瘦的矩阵，例如将 d✖️d 维的稠密矩阵M分解成两个 d✖️r 的高瘦矩阵U和V，当 r << d 时，计算开销将被大大降低，当原来稠密矩阵的奇异值差别较大或快速衰减时，对于计算开销的优化将更加明显。幸运的是，在DCN-v2模型试验中，学习到的稠密矩阵M是一个低秩矩阵，因此很适合做矩阵分解。\n",
    "\n",
    "<img src=\"../data/img/dcn_mix_math.png\" sytle=\"zoom:50%\" />\n",
    "\n",
    "<img src=\"../data/img/dcn_mix.webp\" sytle=\"zoom:50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e93d9f-6294-4da0-a4f4-6baa674236ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
