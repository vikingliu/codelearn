{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f360ef-5d6c-467e-a74a-4cdd11e016be",
   "metadata": {},
   "source": [
    "# FM-隐向量特征交叉\n",
    "\n",
    "$FM = w_0 + \\sum_{i=1}^n w_ix_i + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}x_ix_j$\n",
    "$前两项其实就是一阶加权特征，计算复杂度为O(n)，第三项中的权重 w_{ij}，这儿使用到了矩阵分解，分解为 W=V^TV , v_i、v_j分别为x_i、x_j的隐向量$\n",
    "\n",
    "<img src=\"../data/img/fm.jpeg\" sytle=\"zoom:50%\"/>\n",
    "\n",
    "$FM = w_0 + \\sum_{i=1}^n w_ix_i + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n <v_i, v_j> x_ix_j$\n",
    "\n",
    "$<v_i, v_j> = v_i \\cdot v_j = \\sum_{f=1}^k v_{if}v_{jf}$\n",
    "\n",
    "$\\sum_{i=1}^n \\sum_{j=1}^n = (\\sum_{i=1}^n x_i)^2$\n",
    "\n",
    "我们假设隐向量的长度为k ，那么交叉项的参数量变为 kn 个。此时时间复杂度仍为$O(kn^2)$，通过以下方式可以简化为O(kn)，如下图：\n",
    "\n",
    "<img src=\"../data/img/fm_simple.webp\" sytle=\"zoom:50%\"/>\n",
    "\n",
    "最终公式\n",
    "$FM = w_0 + \\sum_{i=1}^n w_ix_i + \\frac {1}{2} \\sum_{f=1}^k [(\\sum_{i=1}^n v_{if}x_i)^2 - \\sum_{i=1}^n v_{if}^2x_i^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ce6fc-4d4e-4ca7-b99d-724a0862b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FactorizationMachine(nn.Module):\n",
    "    \"\"\"\n",
    "        Factorization Machine\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_fields, embed_dim):\n",
    "        \"\"\"\n",
    "            feature_fileds : array_like\n",
    "                             类别特征的field的数目\n",
    "        \"\"\"\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "\n",
    "        # 输入的是label coder 用输出为1的embedding来形成linear part\n",
    "        self.linear = torch.nn.Embedding(sum(feature_fields) + 1, 1)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((1,)))\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(sum(feature_fields) + 1, embed_dim)\n",
    "        self.offset = np.array((0, *np.cumsum(feature_fields)[:-1]), dtype=np.long)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tmp = x + x.new_tensor(self.offset).unsqueeze(0)  # bs,fields_num [bs,22]\n",
    "\n",
    "        # 线性层\n",
    "        # bs,fields_num,1 -> bs,1 [bs,22,1]->[bs,1]\n",
    "        linear_part = torch.sum(self.linear(tmp), dim=1) + self.bias\n",
    "        # print(\"linear_part shape\", linear_part.shape)\n",
    "\n",
    "        # 内积项\n",
    "        ## embedding\n",
    "        # [bs,1] -> bs,1,embedding_dim [bs,1,8]\n",
    "        tmp = self.embedding(tmp)\n",
    "        ##  XY\n",
    "        # bs,1,embedding_dim -> bs,embedding_dim;; [bs,1,8]->bs,8\n",
    "        square_of_sum = torch.sum(tmp, dim=1) ** 2\n",
    "        # bs,1,embedding_dim -> bs,embedding_dim;; [bs,1,8]->bs,8\n",
    "        sum_of_square = torch.sum(tmp ** 2, dim=1)\n",
    "        # 加权线性层与FM层之和\n",
    "        x = linear_part + 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)\n",
    "        # sigmoid\n",
    "        x = torch.sigmoid(x.squeeze(1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87acde-bdef-4f2f-814a-909e39203892",
   "metadata": {},
   "source": [
    "## FFM\n",
    "\n",
    "FFM在FM的基础上进行改进，提出了特征域的概念，特征域里是同一个特征的不同取值。FM做法对于不同特征交叉认为是同等重要的，然而FFM的理论是不同特征的交叉影响不同，举个简单例子。\n",
    "比如有性别、年龄、职业三种特征，那么在与“职业”中的“清洁工”特征交叉时“男性”的隐向量是 $v_{男性,职业}$，在与“年龄”中的“中年”特征交叉时，“男性”的隐向量是 $v_{男性,年龄}$。\n",
    "这种思维更符合实际场景，不同的特征交叉权重确实应该不同。\n",
    "\n",
    "<img src=\"../data/img/ffm_table.png\"  style=\"zoom:50%\"/>\n",
    "\n",
    "\n",
    "FFM模型认为$v_i$不仅跟$x_i$有关系，还跟与$x_i$相乘的$x_j$所属的Field有关系，即$v_i$成了一个二维向量$v_{F\\times K}$，$F$是Field的总个数。FFM只保留了(FM\n",
    ")中的二次项.\n",
    "\n",
    "$\\hat y =\\sum_{i=1}^n\\sum_{j=i+1}^n v_{i,fj}\\cdot v_{j,fi}x_ix_j$\n",
    "\n",
    "以上文的表格数据为例，计算用户1的$\\hat{y}$\n",
    "\n",
    "$\\hat y =v_{1,f2} \\cdot v_{2,f1} x_1x_2 + v_{1,f3}\\cdot v_{3,f1}x_1x_3 + v_{1,f4}\\cdot v_{4,f1}x_1x_4 + ⋯$\n",
    " \n",
    "\n",
    "由于$x_2,x_3,x_4$属于同一个Field，所以$f2,f3,f4$可以用同一个变量来代替，比如就用$f2$。\n",
    "\n",
    "$\\hat y =v_{1,f2} \\cdot v_{2,f1} x_1x_2 + v_{1,f2}\\cdot v_{3,f1}x_1x_3 + v_{1,f2}\\cdot v_{4,f1}x_1x_4 + ⋯$\n",
    "\n",
    " \n",
    "我们来算一下$\\hat{y}$对$v_{1,f2}$的偏导。\n",
    "\n",
    "$ \\frac {\\partial \\hat y}{\\partial v_{1, f2}} = v_{2,f1}x_1x_2 + v_{3,f1}x_1x_3 + v_{4,f1}x_1x_4$\n",
    "\n",
    "注意$x_2,x_3,x_4$是同一个属性的one-hot表示，即$x_2,x_3,x_4$中只有一个为1，其他都为0。在本例中$x_3=x_4=0, x_2=1$，所以\n",
    "\n",
    "$ \\frac {\\partial \\hat y}{\\partial v_{1, f2}} = v_{2,f1}x_1x_2 $\n",
    "\n",
    "推广到一般情况    \n",
    "$ \\frac {\\partial \\hat y}{\\partial v_{i, fj}} = v_{i,fj}x_ix_kj $\n",
    "\n",
    "\n",
    "$z=\\phi(v,x)=\\sum_{i=1}^n \\sum_{j=i+1}^n v_{i,fj}\\cdot v_{j,fi}x_ix_j$\n",
    "\n",
    "$ \\frac {\\partial z}{\\partial v_{i, fj}} = v_{i,fj}x_ix_kj $\n",
    "\n",
    "$a=\\sigma(z)=\\frac {1}{1+ e^{-z}}= \\frac{1}{1 + e^{-\\phi(v,x)}}$\n",
    "\n",
    "令$y=0$表示负样本，$y=1$表示正样本，$C$表示交叉熵损失函数\n",
    "\n",
    "$κ=\\frac {\\partial C}{\\partial z} = a - y $\n",
    "\n",
    "$缺点: - FFM公式无法化简，计算复杂度较高，FFM需要学习n个特征在f个域上的k维隐向量，参数量nfk个，复杂度 O(kn^2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f312d77-4dd7-449c-bb5b-84eee8b9a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FieldAwareFactorizationMachine(nn.Module):\n",
    "    \"\"\"\n",
    "        FFM \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super(FieldAwareFactorizationMachine, self).__init__()\n",
    "\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "\n",
    "        # 输入的是label coder 用输出为1的embedding来形成linear part\n",
    "        # linear part\n",
    "        self.linear = torch.nn.Embedding(sum(field_dims) + 1, 1)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((1,)))\n",
    "\n",
    "        # ffm part\n",
    "        print(\"field_dims\", field_dims)\n",
    "        self.num_fields = len(field_dims)  # 特征域的数目\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bs,fields_num [bs,22]\n",
    "        tmp = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        # linear part forward\n",
    "        ## bs,fields_num,1 -> bs,1 [bs,22,1]->[bs,1]\n",
    "        linear_part = torch.sum(self.linear(tmp), dim=1) + self.bias\n",
    "        # ffm part forward\n",
    "        # 为每一个field都使用embedding进行映射编码\n",
    "        # 每个embedding中的shape应该为:bs,filed_num,embedding_num -> bs,22,8\n",
    "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]\n",
    "        ix = []\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                # xs[j].shape: torch.Size([2, 22, 8]) bs,field_nums,embedding_num\n",
    "                # xs[j].shape: torch.Size([2, 22, 8])\n",
    "                # xs[j][:, i] shape: torch.Size([2, 8]) bs,embedding_num\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        # print(\"ix len:\",len(ix)) 231\n",
    "        # print(\"ix [0]:\",ix[0].shape) #bs,embdding_num -> bs,8\n",
    "        ix = torch.stack(ix, dim=1)  # ix: -> bs,231,embedding_num\n",
    "        ffm_part = torch.sum(torch.sum(ix, dim=1), dim=1, keepdim=True)  # bs,231,embedding_num -> bs,embedding -> bs,1\n",
    "\n",
    "        x = linear_part + ffm_part\n",
    "        x = torch.sigmoid(x.squeeze(1))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
